"""
Windows-Optimized RAG-Anything Implementation
Complete RAG system optimized for Windows with TinyLlama, embeddings, vector database, and RAG-Anything server integration
"""

import json
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional
import time
import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è FAISS not available. Install with: pip install faiss-cpu")
    FAISS_AVAILABLE = False
    faiss = None
import os
import warnings
import requests

BASE = Path(__file__).resolve().parent      # agent/src
ROOT = BASE.parent 

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

logger = logging.getLogger(__name__)

class RAGAnythingClient:
    """RAG-Anything server client for retrieving additional context"""
    
    def __init__(self, base_url: str = "http://localhost:9999"):
        self.base_url = base_url.rstrip("/")
        self.available = False
        self._test_connection()
    
    def _test_connection(self):
        """Test connection to RAG-Anything server"""
        try:
            resp = requests.get(f"{self.base_url}/health", timeout=5)
            if resp.status_code == 200:
                self.available = True
                logger.info("‚úÖ RAG-Anything server connected")
            else:
                logger.warning("‚ö†Ô∏è RAG-Anything server responded with non-200 status")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è RAG-Anything server unavailable: {e}")
            self.available = False
    
    def retrieve(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        Retrieve context from RAG-Anything server
        
        Args:
            query: Search query
            k: Number of results to retrieve
            
        Returns:
            List of context documents
        """
        if not self.available:
            return []
        
        try:
            resp = requests.post(
                f"{self.base_url}/retrieve", 
                json={"query": query, "k": k}, 
                timeout=10
            )
            resp.raise_for_status()
            data = resp.json()
            
            # Normalize response format
            results = []
            for item in data:
                results.append({
                    "content": item.get("content", ""),
                    "source_id": item.get("source_id", "rag-anything"),
                    "score": item.get("score", 0.0),
                    "title": item.get("title", "RAG-Anything Result"),
                    "source": "rag-anything"
                })
            
            logger.info(f"üìö Retrieved {len(results)} documents from RAG-Anything")
            return results
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error retrieving from RAG-Anything: {e}")
            return []
    
    def is_available(self) -> bool:
        """Check if RAG-Anything server is available"""
        return self.available

class WindowsRAGSystem:
    """Windows-optimized RAG system with TinyLlama, embeddings, and vector search"""
    
    def __init__(self,
             models_dir: str | Path = ROOT / "mobile_models",
             data_dir: str | Path   = ROOT / "mobile_rag_ready",
             embedding_model_name: str = "all-MiniLM-L6-v2",
             rag_anything_url: str = "http://localhost:9999"):

        """
        Initialize Windows-optimized RAG system with RAG-Anything integration
        
        Args:
            models_dir: Directory containing TinyLlama model
            data_dir: Directory containing health guidelines and protocols
            embedding_model_name: Sentence transformer model for embeddings
            rag_anything_url: URL of RAG-Anything server
        """
        self.models_dir = Path(models_dir)
        self.data_dir = Path(data_dir)
        self.embedding_model_name = embedding_model_name
        
        # Initialize components
        self.llm_model = None
        self.llm_tokenizer = None
        self.embedding_model = None
        self.vector_index = None
        self.guidelines = {}
        self.emergency_protocols = {}
        self.doc_ids = []
        
        # Initialize RAG-Anything client
        self.rag_anything_client = RAGAnythingClient(rag_anything_url)
        
        # Windows-specific optimizations
        self._setup_windows_optimizations()
        
        # Load all components
        self._load_health_data()
        self._load_embedding_model()
        self._load_tinyllama_model_windows()
        self._build_vector_index()
        
        logger.info("üè• Windows RAG System Initialized!")
        logger.info(f"üìö Guidelines: {len(self.guidelines)}")
        logger.info(f"üö® Emergency Protocols: {len(self.emergency_protocols)}")
        logger.info(f"ü§ñ TinyLlama Model: {'Loaded' if self.llm_model is not None else 'Not Available'}")
        logger.info(f"üîç Embedding Model: {'Loaded' if self.embedding_model is not None else 'Not Available'}")
        logger.info(f"üìä Vector Index: {'Built' if self.vector_index is not None else 'Not Available'}")
        logger.info(f"üåê RAG-Anything Server: {'Connected' if self.rag_anything_client.is_available() else 'Not Available'}")
    
    def _setup_windows_optimizations(self):
        """Setup Windows-specific optimizations"""
        # Set environment variables for better Windows compatibility
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        os.environ["OMP_NUM_THREADS"] = "4"
        
        # Windows-specific torch settings
        if torch.cuda.is_available():
            logger.info("üöÄ CUDA available - using GPU acceleration")
            self.device = "cuda"
        else:
            logger.info("üíª Using CPU mode")
            self.device = "cpu"
    
    def _load_health_data(self):
        """Load health guidelines and emergency protocols"""
        try:
            # Load guidelines
            guidelines_path = self.data_dir / "processed_guidelines.json"
            if guidelines_path.exists():
                with open(guidelines_path, 'r', encoding='utf-8') as f:
                    self.guidelines = json.load(f)
                logger.info(f"‚úÖ Loaded {len(self.guidelines)} guidelines")
            else:
                logger.warning("‚ö†Ô∏è Guidelines file not found")
            
            # Load emergency protocols
            protocols_path = self.data_dir / "emergency_protocols.json"
            if protocols_path.exists():
                with open(protocols_path, 'r', encoding='utf-8') as f:
                    self.emergency_protocols = json.load(f)
                logger.info(f"‚úÖ Loaded {len(self.emergency_protocols)} emergency protocols")
            else:
                logger.warning("‚ö†Ô∏è Emergency protocols file not found")
                
        except Exception as e:
            logger.error(f"‚ùå Error loading health data: {e}")
    
    def _load_embedding_model(self):
        """Load sentence transformer model for embeddings"""
        try:
            logger.info("üîÑ Loading embedding model...")
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            logger.info("‚úÖ Embedding model loaded successfully")
        except Exception as e:
            logger.error(f"‚ùå Error loading embedding model: {e}")
            self.embedding_model = None
    
    def _load_tinyllama_model_windows(self):
        """Load Qwen2.5-0.5B-Instruct model optimized for Windows (replacing TinyLlama)"""
        try:
            # Find model path - prioritize Qwen2.5-0.5B-Instruct
            possible_paths = [
                # Qwen2.5-0.5B-Instruct paths (prioritized)
                Path("../../../agent/mobile_models/qwen2_5_0_5b"),
                Path("../mobile_models/qwen2_5_0_5b"),
                Path("../../agent/mobile_models/qwen2_5_0_5b"),
                self.models_dir / "qwen2_5_0_5b",
                Path("mobile_models/qwen2_5_0_5b"),
                Path("../agent/mobile_models/qwen2_5_0_5b"),
                Path("../../mobile_models/qwen2_5_0_5b"),
                # Fallback to TinyLlama if Qwen not found
                Path("../mobile_models/quantized_tinyllama_health"),
                Path("../../agent/mobile_models/quantized_tinyllama_health"),
                self.models_dir / "quantized_tinyllama_health",
                Path("mobile_models/quantized_tinyllama_health"),
                Path("../agent/mobile_models/quantized_tinyllama_health"),
                Path("../../mobile_models/quantized_tinyllama_health")
            ]
            
            model_path = None
            for path in possible_paths:
                if path.exists():
                    model_path = path
                    break
            
            if model_path is None:
                logger.warning("‚ö†Ô∏è No suitable model found, using fallback responses")
                return
            
            # Check if it's Qwen, Phi-3-mini, or TinyLlama
            is_qwen = "qwen" in str(model_path).lower()
            is_phi3 = "phi3" in str(model_path).lower()
            
            if is_qwen:
                model_name = "Qwen2.5-0.5B-Instruct"
            elif is_phi3:
                model_name = "Phi-3-mini"
            else:
                model_name = "TinyLlama"
            
            logger.info(f"üîÑ Loading {model_name} model (Windows optimized)...")
            
            # Load tokenizer
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                str(model_path),
                trust_remote_code=True
            )
            
            # Windows-optimized model loading
            if is_qwen:
                # Qwen2.5-0.5B-Instruct loading (non-quantized, should work perfectly)
                logger.info("üîÑ Loading Qwen2.5-0.5B-Instruct model...")
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    str(model_path),
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map="auto" if self.device == "cuda" else "cpu",
                    low_cpu_mem_usage=True,
                    use_safetensors=True
                )
            elif is_phi3:
                # Phi-3-mini loading (non-quantized, should work well)
                logger.info("üîÑ Loading Phi-3-mini model...")
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    str(model_path),
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map="auto" if self.device == "cuda" else "cpu",
                    low_cpu_mem_usage=True,
                    use_safetensors=True
                )
            else:
                # TinyLlama loading (with fallback approaches for quantized models)
                logger.info("üîÑ Loading TinyLlama model with fallback approaches...")
                if self.device == "cuda":
                    # GPU loading with quantization support
                    self.llm_model = AutoModelForCausalLM.from_pretrained(
                        str(model_path),
                        trust_remote_code=True,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        low_cpu_mem_usage=True,
                        use_safetensors=True
                    )
                else:
                    # CPU loading without quantization issues
                    self.llm_model = AutoModelForCausalLM.from_pretrained(
                        str(model_path),
                        trust_remote_code=True,
                        torch_dtype=torch.float32,
                        device_map="cpu",
                        low_cpu_mem_usage=True,
                        use_safetensors=True,
                        load_in_8bit=False,
                        load_in_4bit=False
                    )
            
            # Set pad token
            if self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token
            
            logger.info(f"‚úÖ {model_name} model loaded successfully ({self.device} mode)")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Model loading failed: {e}")
            logger.info("üí° Using rule-based responses instead of AI-generated text")
            self.llm_model = None
            self.llm_tokenizer = None
    
    def _build_vector_index(self):
        """Load pre-built vector database or build from scratch if not available"""
        try:
            # First, try to load pre-built vector database
            vector_db_path = self.data_dir / "vector_database.pkl"
            if vector_db_path.exists():
                logger.info("üîÑ Loading pre-built vector database...")
                with open(vector_db_path, 'rb') as f:
                    vector_data = pickle.load(f)
                
                # Extract vector index and document IDs from pre-built database
                if isinstance(vector_data, dict) and 'index' in vector_data and 'doc_ids' in vector_data:
                    self.vector_index = vector_data['index']
                    self.doc_ids = vector_data['doc_ids']
                    logger.info(f"‚úÖ Pre-built vector database loaded with {len(self.doc_ids)} documents")
                    return
                else:
                    logger.warning("‚ö†Ô∏è Pre-built vector database format not recognized, building from scratch...")
            
            # Check if FAISS is available
            if not FAISS_AVAILABLE:
                logger.warning("‚ö†Ô∏è FAISS not available. Vector search will use simple keyword matching.")
                self.vector_index = None
                return
            
            # Fallback: Build vector index from scratch if pre-built database not available
            if not self.embedding_model or not self.guidelines:
                logger.warning("‚ö†Ô∏è Cannot build vector index: missing embedding model or guidelines")
                return
            
            logger.info("üîÑ Building vector index from scratch...")
            
            # Prepare documents
            documents = []
            doc_ids = []
            
            for guideline_id, guideline in self.guidelines.items():
                content = guideline.get("content", "")
                if content:
                    documents.append(content)
                    doc_ids.append(guideline_id)
            
            if not documents:
                logger.warning("‚ö†Ô∏è No documents to index")
                return
            
            # Generate embeddings
            embeddings = self.embedding_model.encode(documents)
            
            # Create FAISS index
            dimension = embeddings.shape[1]
            self.vector_index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
            
            # Normalize embeddings for cosine similarity
            faiss.normalize_L2(embeddings)
            self.vector_index.add(embeddings.astype('float32'))
            
            # Store document IDs
            self.doc_ids = doc_ids
            
            logger.info(f"‚úÖ Vector index built with {len(documents)} documents")
            
        except Exception as e:
            logger.error(f"‚ùå Error building vector index: {e}")
            self.vector_index = None
    
    def _generate_response(self, prompt: str, max_length: int = 75) -> str:
        """Generate response using TinyLlama model"""
        if self.llm_model is None or self.llm_tokenizer is None:
            return "Model not available for text generation."
        
        try:
            # Tokenize input
            inputs = self.llm_tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=512
            )
            
            # Move to device
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Generate response with strict parameters for concise output
            with torch.no_grad():
                outputs = self.llm_model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    temperature=0.3,  # Lower temperature for more focused responses
                    do_sample=True,
                    pad_token_id=self.llm_tokenizer.eos_token_id,
                    eos_token_id=self.llm_tokenizer.eos_token_id,
                    repetition_penalty=1.2,  # Higher repetition penalty
                    use_cache=False,  # Disable cache for better control
                    num_beams=1,  # Greedy decoding for consistency
                    early_stopping=True  # Stop early when EOS is reached
                )
            
            # Decode response
            response = self.llm_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response: {e}")
            return "Error generating response."
    
    def _vector_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Perform vector search on health guidelines"""
        if not self.vector_index or not self.embedding_model:
            # Fallback to simple keyword matching if vector search not available
            return self._simple_keyword_search(query, k)
        
        try:
            # Generate query embedding
            query_embedding = self.embedding_model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            # Search vector index
            scores, indices = self.vector_index.search(query_embedding.astype('float32'), k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.doc_ids):
                    guideline_id = self.doc_ids[idx]
                    guideline = self.guidelines.get(guideline_id, {})
                    
                    results.append({
                        "guideline_id": guideline_id,
                        "title": guideline.get("title", "Unknown"),
                        "content": guideline.get("content", ""),
                        "score": float(score),
                        "emergency_level": guideline.get("emergency_level", "medium"),
                        "source": "local_vector_db"
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Error in vector search: {e}")
            # Fallback to simple keyword matching
            return self._simple_keyword_search(query, k)
    
    def _simple_keyword_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Simple keyword-based search as fallback when vector search is not available"""
        query_lower = query.lower()
        results = []
        
        for guideline_id, guideline in self.guidelines.items():
            content = guideline.get("content", "").lower()
            title = guideline.get("title", "").lower()
            
            # Simple keyword matching
            score = 0
            for word in query_lower.split():
                if word in content:
                    score += 1
                if word in title:
                    score += 2  # Title matches are more important
            
            if score > 0:
                results.append({
                    "guideline_id": guideline_id,
                    "title": guideline.get("title", "Unknown"),
                    "content": guideline.get("content", ""),
                    "score": float(score) / 10.0,  # Normalize score
                    "emergency_level": guideline.get("emergency_level", "medium"),
                    "source": "keyword_search"
                })
        
        # Sort by score and return top k results
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:k]
    
    def _hybrid_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Perform hybrid search combining local vector search and RAG-Anything"""
        all_results = []
        
        # 1. Local vector search
        local_results = self._vector_search(query, k)
        all_results.extend(local_results)
        
        # 2. RAG-Anything search
        if self.rag_anything_client.is_available():
            rag_results = self.rag_anything_client.retrieve(query, k)
            all_results.extend(rag_results)
        else:
            logger.info("üåê RAG-Anything server not available, using local results only")
        
        # 3. Combine and deduplicate results
        combined_results = self._combine_and_rank_results(all_results, query)
        
        return combined_results[:k]
    
    def _combine_and_rank_results(self, results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """Combine and rank results from different sources"""
        # Remove duplicates based on content similarity
        unique_results = []
        seen_contents = set()
        
        for result in results:
            content = result.get("content", "")
            # Simple deduplication based on content hash
            content_hash = hash(content[:100])  # Use first 100 chars for deduplication
            
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_results.append(result)
        
        # Sort by score (if available) or by source priority
        def sort_key(result):
            score = result.get("score", 0.0)
            source = result.get("source", "unknown")
            
            # Prioritize emergency protocols and local results
            if source == "emergency_protocols":
                return (1.0, score)
            elif source == "local_vector_db":
                return (0.8, score)
            elif source == "rag-anything":
                return (0.6, score)
            else:
                return (0.4, score)
        
        return sorted(unique_results, key=sort_key, reverse=True)
    
    def _detect_emergency_type(self, query: str) -> Optional[str]:
        """Detect emergency type from query"""
        query_lower = query.lower()
        
        emergency_keywords = {
            "chest_pain": ["chest pain", "heart attack", "cardiac", "heart", "chest"],
            "fainting": ["fainted", "fainting", "unconscious", "passed out", "collapsed"],
            "burn": ["burn", "burned", "fire", "hot", "scald", "thermal"],
            "choking": ["choking", "can't breathe", "blocked airway", "suffocating"],
            "stroke": ["stroke", "facial droop", "slurred speech", "weakness", "paralysis"],
            "shortness_breath": ["shortness of breath", "can't breathe", "breathing difficulty", "dyspnea"]
        }
        
        for emergency_type, keywords in emergency_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                return emergency_type
        
        return None
    
    def _create_rag_prompt(self, query: str, context: List[Dict[str, Any]]) -> str:
        """Create concise RAG prompt optimized for emergency detection and call-to-action"""
        # Extract key context (much shorter)
        context_text = ""
        for item in context[:2]:  # Only use top 2 results
            title = item.get("title", "Health Info")
            context_text += f"- {title}\n"
        
        prompt = f"""Emergency: "{query}"

{context_text}

Emergency: Yes/No. Action: [seek hospital/see doctor]. Include key symptoms if specific. 25 words max."""
        
        return prompt
    
    def query_health_emergency(self, query: str) -> Dict[str, Any]:
        """Main query function for health emergencies with RAG"""
        start_time = time.time()
        
        try:
            logger.info(f"üö® Health Emergency Query: {query[:100]}...")
            
            # Detect emergency type
            emergency_type = self._detect_emergency_type(query)
            
            # Get relevant information
            if emergency_type and emergency_type in self.emergency_protocols:
                # Emergency protocol response
                protocol = self.emergency_protocols[emergency_type]
                
                # Get additional context from vector search
                vector_results = self._vector_search(query, k=3)
                
                # Generate AI response with context
                ai_response = None
                if self.llm_model is not None:
                    prompt = self._create_rag_prompt(query, vector_results)
                    ai_response = self._generate_response(prompt, max_length=150)
                
                response = {
                    "emergency_type": emergency_type,
                    "protocol": protocol,
                    "immediate_actions": protocol.get("immediate_actions", [])[:3],
                    "warning_signs": protocol.get("warning_signs", []),
                    "call_911": protocol.get("call_911", True),
                    "confidence": 0.9,
                    "source": "emergency_protocols",
                    "vector_results": vector_results,
                    "ai_response": ai_response,
                    "processing_time": time.time() - start_time
                }
            else:
                # General health query with hybrid RAG search
                vector_results = self._hybrid_search(query, k=5)
                
                # Generate AI response with context
                ai_response = None
                if self.llm_model is not None:
                    prompt = self._create_rag_prompt(query, vector_results)
                    ai_response = self._generate_response(prompt, max_length=150)
                
                response = {
                    "emergency_type": "general_health",
                    "vector_results": vector_results,
                    "call_911": any(r.get("emergency_level") == "critical" for r in vector_results),
                    "confidence": max([r.get("score", 0) for r in vector_results], default=0.3),
                    "source": "rag_search",
                    "ai_response": ai_response,
                    "processing_time": time.time() - start_time
                }
            
            # Add natural language response
            response["natural_response"] = self._format_natural_response(response, query)
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Error in health emergency query: {e}")
            return {
                "emergency_type": "error",
                "error": str(e),
                "call_911": True,
                "confidence": 0.0,
                "processing_time": time.time() - start_time,
                "natural_response": "I'm unable to process this health emergency. Please call 911 immediately."
            }
    
    def _format_natural_response(self, response: Dict[str, Any], query: str) -> str:
        """Format natural language response"""
        # Use AI response if available
        if response.get("ai_response") and response["ai_response"] != "Model not available for text generation.":
            return response["ai_response"]
        
        # Fallback to rule-based responses
        emergency_type = response.get("emergency_type", "general_health")
        call_911 = response.get("call_911", False)
        
        if emergency_type == "chest_pain":
            if call_911:
                return "Based on your symptoms, this appears to be a potential heart attack or cardiac emergency. Chest pain with breathing difficulties is a serious medical emergency that requires immediate attention. You should call 911 right away and try to stay calm while waiting for help."
            else:
                return "Your chest pain symptoms could indicate several conditions ranging from heartburn to anxiety. However, any chest pain should be taken seriously and evaluated by a healthcare provider. Monitor your symptoms closely and seek medical attention if they worsen."
        
        elif emergency_type == "shortness_breath":
            return "Your symptoms of shortness of breath could indicate several serious conditions including respiratory problems, heart issues, or shock. These symptoms suggest your body may not be getting enough oxygen, which is a medical emergency. You should call 911 immediately and try to stay calm while waiting for help."
        
        elif emergency_type == "fainting":
            if call_911:
                return "Fainting with potential head injury is a serious medical emergency that requires immediate attention. Loss of consciousness can indicate various serious conditions including head trauma, cardiac issues, or neurological problems. Call 911 immediately and while waiting, check if the person is breathing."
            else:
                return "Fainting episodes can have various causes including dehydration, low blood pressure, or stress. However, any loss of consciousness should be evaluated by a healthcare provider to rule out serious conditions. Monitor the person closely and seek medical attention if symptoms persist."
        
        elif emergency_type == "choking":
            return "Choking is a life-threatening emergency that requires immediate action. When someone cannot speak or breathe due to a blocked airway, every second counts. Call 911 immediately and perform the Heimlich maneuver if you're trained to do so, or encourage the person to cough forcefully."
        
        elif emergency_type == "stroke":
            return "Facial drooping is a classic sign of stroke, which is a medical emergency that requires immediate treatment. Time is critical with strokes - the sooner treatment begins, the better the outcome. Call 911 immediately and note the time when symptoms started, as this information is crucial for treatment decisions."
        
        else:
            # General health response with context
            vector_results = response.get("vector_results", [])
            if vector_results:
                best_result = vector_results[0]
                content = best_result.get("content", "")
                if len(content) > 200:
                    content = content[:200] + "..."
                return f"Based on your symptoms, here's relevant health information: {content}"
            else:
                return "Your symptoms require medical attention and should be evaluated by a healthcare provider. While I cannot provide a specific diagnosis, it's important to take your symptoms seriously. If you're experiencing severe or worsening symptoms, call 911 or seek immediate medical care."
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get system status"""
        return {
            "guidelines_loaded": len(self.guidelines),
            "emergency_protocols": len(self.emergency_protocols),
            "llm_model_loaded": self.llm_model is not None,
            "embedding_model_loaded": self.embedding_model is not None,
            "vector_index_built": self.vector_index is not None,
            "rag_anything_available": self.rag_anything_client.is_available(),
            "rag_anything_url": self.rag_anything_client.base_url,
            "device": self.device,
            "system_ready": True
        }


def test_windows_rag_system():
    """Test the Windows RAG system"""
    print("üß™ Testing Windows RAG System")
    print("=" * 60)
    
    # Configure logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    try:
        # Initialize the Windows RAG system
        print("üîß Initializing Windows RAG System...")
        rag_system = WindowsRAGSystem()
        
        # Check system status
        status = rag_system.get_system_status()
        print(f"\nüìä System Status:")
        print(f"   Guidelines: {status['guidelines_loaded']}")
        print(f"   Emergency Protocols: {status['emergency_protocols']}")
        print(f"   LLM Model: {'‚úÖ' if status['llm_model_loaded'] else '‚ùå'}")
        print(f"   Embedding Model: {'‚úÖ' if status['embedding_model_loaded'] else '‚ùå'}")
        print(f"   Vector Index: {'‚úÖ' if status['vector_index_built'] else '‚ùå'}")
        print(f"   RAG-Anything Server: {'‚úÖ' if status['rag_anything_available'] else '‚ùå'} ({status['rag_anything_url']})")
        print(f"   Device: {status['device']}")
        
        # Test queries
        test_queries = [
            "I have severe chest pain and can't breathe properly",
            "Someone just fainted and hit their head",
            "A person is choking on food and can't speak",
            "My neighbor is showing signs of stroke with facial drooping",
            "I have shortness of breath, pale skin, and cold skin"
        ]
        
        print(f"\nüö® Testing Health Emergency Queries:")
        for i, query in enumerate(test_queries, 1):
            print(f"\n{i}. Query: {query}")
            print("-" * 50)
            
            result = rag_system.query_health_emergency(query)
            
            print(f"ü§ñ NATURAL RESPONSE:")
            print(f"   {result.get('natural_response', 'No response available')}")
            
            print(f"\nüìã TECHNICAL ANALYSIS:")
            print(f"   Emergency Type: {result.get('emergency_type', 'Unknown').replace('_', ' ').title()}")
            print(f"   Call 911: {'YES - IMMEDIATELY' if result.get('call_911') else 'NO - Monitor situation'}")
            print(f"   Confidence: {result.get('confidence', 0.0):.1%}")
            print(f"   Processing Time: {result.get('processing_time', 0.0):.2f}s")
            
            # Show immediate actions if available
            immediate_actions = result.get('immediate_actions', [])
            if immediate_actions:
                print(f"\n‚ö° IMMEDIATE ACTIONS:")
                for j, action in enumerate(immediate_actions, 1):
                    print(f"   {j}. {action}")
            
            # Show vector search results if available
            vector_results = result.get('vector_results', [])
            if vector_results:
                print(f"\nüìö RELEVANT HEALTH INFORMATION:")
                for j, result_item in enumerate(vector_results[:2], 1):
                    content = result_item.get('content', '')
                    if len(content) > 150:
                        content = content[:150] + "..."
                    print(f"   {j}. {content}")
                    print(f"      Source: {result_item.get('title', 'Health Guidelines')}")
        
        print(f"\n‚úÖ Windows RAG System Test Completed!")
        print(f"üí° The system is ready for health emergency assistance on Windows!")
        
    except Exception as e:
        print(f"‚ùå Error testing Windows RAG system: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_windows_rag_system()
